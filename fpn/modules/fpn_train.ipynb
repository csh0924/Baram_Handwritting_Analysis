{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84afb8f-db09-44c1-9c24-1715a65210e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "import importlib, sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from fpn_common import (\n",
    "    setup_project_path, get_device,\n",
    "    imread_unicode,\n",
    "    ResNetFPN, imagenet_normalize_chw,\n",
    "    assert_saved_matches_backbone,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03d5dc4-49a2-4a70-a392-7528ec37ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = setup_project_path()\n",
    "BASE_DIR = project_root / \"hangul_dataset\" / \"korean_generated\"\n",
    "IMAGES_DIR = BASE_DIR / \"images\"\n",
    "LABELS_DIR = BASE_DIR / \"labels\"\n",
    "\n",
    "\n",
    "TYPE_FOLDERS = [\n",
    "    \"vertical_no_jong\",\n",
    "    \"complex_no_jong\",\n",
    "    \"horizontal_no_jong\",\n",
    "    \"complex_jong\",\n",
    "    \"horizontal_jong\",\n",
    "    \"vertical_jong\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2338773-b995-4e6b-bc68-d7daa3b7db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augment_cv2(\n",
    "    img_rgb: np.ndarray,\n",
    "    lbl: np.ndarray,\n",
    "    rng: np.random.Generator,\n",
    "    rotate_deg: float = 8.0,\n",
    "    scale_range: Tuple[float, float] = (0.9, 1.1),\n",
    "    translate_frac: float = 0.06,\n",
    "    hflip_p: float = 0.5,\n",
    "    brightness: float = 0.15,\n",
    "    contrast: float = 0.15,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    H, W = lbl.shape[:2]\n",
    "\n",
    "    angle = float(rng.uniform(-rotate_deg, rotate_deg))\n",
    "    scale = float(rng.uniform(scale_range[0], scale_range[1]))\n",
    "    tx = float(rng.uniform(-translate_frac, translate_frac) * W)\n",
    "    ty = float(rng.uniform(-translate_frac, translate_frac) * H)\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((W * 0.5, H * 0.5), angle, scale)\n",
    "    M[0, 2] += tx\n",
    "    M[1, 2] += ty\n",
    "\n",
    "    img_aug = cv2.warpAffine(\n",
    "        img_rgb, M, (W, H),\n",
    "        flags=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=(0, 0, 0),\n",
    "    )\n",
    "    lbl_aug = cv2.warpAffine(\n",
    "        lbl, M, (W, H),\n",
    "        flags=cv2.INTER_NEAREST,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=0,\n",
    "    )\n",
    "\n",
    "    if rng.random() < hflip_p:\n",
    "        img_aug = np.ascontiguousarray(img_aug[:, ::-1, :])\n",
    "        lbl_aug = np.ascontiguousarray(lbl_aug[:, ::-1])\n",
    "\n",
    "    img_f = img_aug.astype(np.float32) / 255.0\n",
    "    c = float(rng.uniform(1.0 - contrast, 1.0 + contrast))\n",
    "    b = float(rng.uniform(-brightness, brightness))\n",
    "    img_f = (img_f - 0.5) * c + 0.5 + b\n",
    "    img_f = np.clip(img_f, 0.0, 1.0)\n",
    "\n",
    "    img_out = (img_f * 255.0).astype(np.uint8)\n",
    "    lbl_out = lbl_aug.astype(lbl.dtype)\n",
    "    return img_out, lbl_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b7a63e-eacd-40e2-bb1e-b804502b1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangulTypeSegDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        type_name: str,\n",
    "        size: Tuple[int, int] = (256, 256),\n",
    "        augment: bool = False,\n",
    "        seed: int = 42,\n",
    "        aug_cfg: Optional[Dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.type_name = type_name\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.seed = seed\n",
    "        self.aug_cfg = aug_cfg or {}\n",
    "\n",
    "        self.img_dir = IMAGES_DIR / type_name\n",
    "        self.lbl_dir = LABELS_DIR / type_name\n",
    "\n",
    "        self.files = sorted([p.name for p in self.lbl_dir.iterdir() if p.is_file()])\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No label files in {self.lbl_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn = self.files[idx]\n",
    "        img_path = self.img_dir / fn\n",
    "        lbl_path = self.lbl_dir / fn\n",
    "\n",
    "        img_bgr = imread_unicode(img_path, cv2.IMREAD_COLOR)\n",
    "        if img_bgr is None:\n",
    "            raise RuntimeError(f\"Failed to read image: {img_path}\")\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb = cv2.resize(img_rgb, (self.size[1], self.size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        lbl = imread_unicode(lbl_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if lbl is None:\n",
    "            raise RuntimeError(f\"Failed to read label: {lbl_path}\")\n",
    "        lbl = cv2.resize(lbl, (self.size[1], self.size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if self.augment:\n",
    "            rng = np.random.default_rng(self.seed * 1000003 + idx)\n",
    "            img_rgb, lbl = apply_augment_cv2(img_rgb, lbl, rng, **self.aug_cfg)\n",
    "\n",
    "        img = img_rgb.astype(np.float32) / 255.0\n",
    "        lbl = lbl.astype(np.int64)\n",
    "\n",
    "        x = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        x = imagenet_normalize_chw(x)\n",
    "        y = torch.from_numpy(lbl)\n",
    "\n",
    "        return x, y, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be72fdd-fa34-4700-b606-9d2ca2cf6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, device, criterion: nn.Module) -> float:\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "    return total / max(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c31c794-41e6-4e42-9bf1-9b93680541e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_type(\n",
    "    type_name: str,\n",
    "    out_dir: Path,\n",
    "    size=(256, 256),\n",
    "    batch_size: int = 8,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 1e-3,\n",
    "    val_ratio: float = 0.1,\n",
    "    num_workers: int = 0,\n",
    "    backbone: str = \"resnet34\",\n",
    "    pretrained_backbone: bool = True,\n",
    "    seed: int = 42,\n",
    "    use_augmentation: bool = True,\n",
    "    aug_cfg: Optional[Dict] = None,\n",
    "    plot_loss: bool = True,\n",
    "    save_plot: bool = True,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    base_ds = HangulTypeSegDataset(type_name=type_name, size=size, augment=False, seed=seed)\n",
    "    n_total = len(base_ds)\n",
    "    n_val = max(1, int(n_total * val_ratio))\n",
    "    n_train = n_total - n_val\n",
    "\n",
    "    all_idx = torch.randperm(n_total, generator=g).tolist()\n",
    "    val_idx = all_idx[:n_val]\n",
    "    train_idx = all_idx[n_val:]\n",
    "\n",
    "    train_ds_full = HangulTypeSegDataset(\n",
    "        type_name=type_name,\n",
    "        size=size,\n",
    "        augment=bool(use_augmentation),\n",
    "        seed=seed,\n",
    "        aug_cfg=aug_cfg or dict(\n",
    "            rotate_deg=8.0,\n",
    "            scale_range=(0.9, 1.1),\n",
    "            translate_frac=0.06,\n",
    "            hflip_p=0.5,\n",
    "            brightness=0.15,\n",
    "            contrast=0.15,\n",
    "        ),\n",
    "    )\n",
    "    val_ds_full = HangulTypeSegDataset(type_name=type_name, size=size, augment=False, seed=seed)\n",
    "\n",
    "    train_ds = Subset(train_ds_full, train_idx)\n",
    "    val_ds = Subset(val_ds_full, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)\n",
    "\n",
    "    device = get_device()\n",
    "    model = ResNetFPN(num_classes=4, backbone=backbone, pretrained=pretrained_backbone).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_path = out_dir / f\"fpn_{type_name}_{backbone}.pth\"\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Train type: {type_name}\")\n",
    "    print(f\" total={n_total}, train={n_train}, val={n_val}\")\n",
    "    print(f\" aug={use_augmentation} | save -> {best_path}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running, seen = 0.0, 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[{type_name}] Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for x, y, _ in pbar:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running += loss.item() * x.size(0)\n",
    "            seen += x.size(0)\n",
    "            pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        train_loss = running / max(seen, 1)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, device, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"[{type_name}] Epoch {epoch:03d}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            assert_saved_matches_backbone(best_path, backbone)\n",
    "\n",
    "    print(f\"\\n Done: {type_name}\")\n",
    "    print(f\"Best val_loss = {best_val:.4f}\")\n",
    "    print(f\"Saved to: {best_path}\")\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Loss Curve - {type_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        if save_plot:\n",
    "            plot_path = out_dir / f\"loss_{type_name}_{backbone}.png\"\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(val_losses, label=\"Val Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(f\"Loss Curve - {type_name}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(plot_path, dpi=150)\n",
    "            plt.close()\n",
    "            print(f\"Loss plot saved: {plot_path}\")\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c6c2b5-ece0-481f-ab63-e90930a83085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_types(\n",
    "    out_dir: Path,\n",
    "    backbone: str = \"resnet34\",\n",
    "    pretrained_backbone: bool = True,\n",
    "    size=(256, 256),\n",
    "    batch_size=2,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    val_ratio=0.1,\n",
    "    num_workers=0,\n",
    "    seed=42,\n",
    "    use_augmentation=True,\n",
    "    aug_cfg=None,\n",
    "):\n",
    "    saved = {}\n",
    "    for t in TYPE_FOLDERS:\n",
    "        best_path = train_one_type(\n",
    "            type_name=t,\n",
    "            out_dir=out_dir,\n",
    "            size=size,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            val_ratio=val_ratio,\n",
    "            num_workers=num_workers,\n",
    "            backbone=backbone,\n",
    "            pretrained_backbone=pretrained_backbone,\n",
    "            seed=seed,\n",
    "            use_augmentation=use_augmentation,\n",
    "            aug_cfg=aug_cfg,\n",
    "            plot_loss=True,\n",
    "            save_plot=True,\n",
    "        )\n",
    "        saved[t] = best_path\n",
    "    return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284f08d-580a-427f-a8e0-363ec7699591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Train type: vertical_no_jong\n",
      " total=2880, train=2592, val=288\n",
      " aug=True | save -> D:\\Study\\학교강의\\4학년2학기\\캡스톤\\Baram_Handwritting_Analysis\\fpn\\weights\\resnet34_run\\fpn_vertical_no_jong_resnet34.pth\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe58571857c044749dc7f9e207c0d823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[vertical_no_jong] Epoch 1/10:   0%|          | 0/1296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OUT_DIR = project_root / \"fpn\" / \"weights\" / \"resnet34_run\"\n",
    "    AUG_CFG = dict(\n",
    "        rotate_deg=2.0,\n",
    "        scale_range=(0.8, 1.1),\n",
    "        translate_frac=0.06,\n",
    "        hflip_p=0.0,\n",
    "        brightness=0.0,\n",
    "        contrast=0.0,\n",
    "    )\n",
    "\n",
    "    saved = train_all_types(\n",
    "        out_dir=OUT_DIR,\n",
    "        backbone=\"resnet34\",\n",
    "        pretrained_backbone=True,\n",
    "        size=(256, 256),\n",
    "        batch_size=2,\n",
    "        epochs=10,\n",
    "        lr=1e-3,\n",
    "        val_ratio=0.1,\n",
    "        num_workers=0,\n",
    "        seed=42,\n",
    "        use_augmentation=True,\n",
    "        aug_cfg=AUG_CFG,\n",
    "    )\n",
    "\n",
    "    print(\"\\nSaved weights:\")\n",
    "    for k, v in saved.items():\n",
    "        print(k, \"->\", v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-craft]",
   "language": "python",
   "name": "conda-env-.conda-craft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
